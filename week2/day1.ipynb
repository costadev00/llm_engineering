{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBH\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because she discovered he was always trying to \"normalize\" their relationship!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they couldnâ€™t find any significant correlation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go to therapy?  \n",
      "\n",
      "Because they had too many unresolved issues and couldn't find the right model to fit!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was too \"cell-fish\" and couldn't handle her array of emotions!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here to tell jokes, not provide current date and time information. But here's a time-related joke for you:\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "If you need the actual date and time in London, you might want to check your device's clock or do a quick internet search. But if you'd like to hear more jokes, I'd be happy to share some!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "# user_prompt = \"What is today's date and time in London?\"\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a joke-telling assistant, so while I'd love to crack a joke about time zones, I don't actually have access to real-time information like the current date and time in London. I don't have internet access or the ability to check current data.\n",
      "\n",
      "If you're looking for a joke about London time though:\n",
      "\n",
      "Why are London clocks the most humble timepieces in the world?\n",
      "Because they're always standing by Big Ben, and anyone would feel small in comparison!"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The API for Gemini has a slightly different structure.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m gemini = \u001b[43mgoogle\u001b[49m.generativeai.GenerativeModel(\n\u001b[32m      6\u001b[39m     model_name=\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     system_instruction=system_message\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m response = gemini.generate_content(user_prompt)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[31mNameError\u001b[39m: name 'google' is not defined"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down how to decide if a business problem is a good fit for a Large Language Model (LLM) solution. It's not just about whether an LLM *can* touch the problem, but whether it's the *best, most efficient, and appropriate* tool.\n",
      "\n",
      "Here are key questions and criteria to consider:\n",
      "\n",
      "## 1. Does the Problem Fundamentally Involve Language or Text?\n",
      "\n",
      "*   **Yes:** If the core task is understanding, generating, summarizing, translating, analyzing, or manipulating human language (text), an LLM is likely a potential fit.\n",
      "    *   *Examples:* Customer support response generation, content creation, document summarization, sentiment analysis, chatbot interaction, extracting information from contracts, translating emails.\n",
      "*   **No:** If the problem is primarily about complex numerical calculation, structured data processing, database lookups, real-time control of physical systems, or tasks that don't involve significant text processing, an LLM is probably not the primary solution (though it might be a small component, e.g., generating a text summary of structured data).\n",
      "    *   *Examples:* Calculating payroll, managing inventory levels, controlling a robotic arm, processing financial transactions (beyond generating summaries or explanations), complex engineering simulations.\n",
      "\n",
      "**This is your first and most important filter.**\n",
      "\n",
      "## 2. What Level of Accuracy and Determinism is Required?\n",
      "\n",
      "*   **High Tolerance for Variability/Potential Error:** If the task allows for occasional errors, non-deterministic outputs (different answers to the same prompt), or outputs that are \"good enough\" rather than perfectly precise, LLMs can be suitable. You'll need mechanisms to review or guardrail outputs.\n",
      "    *   *Examples:* Drafting creative copy, suggesting email responses, summarizing discussion threads, brainstorming ideas.\n",
      "*   **Need for High Accuracy and Determinism:** If the task requires absolute factual accuracy, precise calculations, or consistent, predictable outputs every time, relying *solely* on a general LLM is risky due to hallucinations and variability. You'll need significant post-processing, verification steps, or augmentation with other tools (like retrieval-augmented generation or function calling).\n",
      "    *   *Examples:* Generating legal contracts, providing critical medical advice, calculating financial figures, writing executable code without verification, generating factual reports that must be 100% correct.\n",
      "\n",
      "## 3. How Complex is the Logic or Reasoning Required?\n",
      "\n",
      "*   **Processing and Synthesizing Text Information:** LLMs excel at pattern matching, synthesizing information *from the text they are given*, and following instructions based on that text.\n",
      "    *   *Examples:* Answering questions based on a document, summarizing multiple documents, identifying entities in text.\n",
      "*   **Complex, Multi-Step Logical Reasoning or External Knowledge:** LLMs struggle with complex multi-step deduction, reasoning *about* the real world (beyond their training data), or tasks that require interacting precisely with external systems or databases *without* specific tools (like function calling or agents).\n",
      "    *   *Examples:* Solving novel logical puzzles, providing step-by-step procedural instructions for an unfamiliar task, integrating information from disparate, real-time external systems without an agentic framework.\n",
      "\n",
      "## 4. What are the Data Requirements?\n",
      "\n",
      "*   **Availability of Relevant Text Data:** Do you have access to the kind of text data the LLM will need to process or be fine-tuned on? Is it clean and relevant?\n",
      "    *   *Consider:* Customer conversations, internal documents, industry reports, product descriptions, etc.\n",
      "*   **Data Sensitivity and Privacy:** LLMs, especially external APIs, pose risks for sensitive data. Consider data anonymization, deploying models privately (on-prem or in your cloud), or using models specifically designed for privacy.\n",
      "    *   *High Sensitivity:* Financial data, PII (Personally Identifiable Information), health records, confidential company strategy. Need robust security/privacy plan.\n",
      "    *   *Low Sensitivity:* Publicly available text, anonymized data. Easier to use external services.\n",
      "\n",
      "## 5. What are the Performance and Latency Needs?\n",
      "\n",
      "*   **Acceptable Latency:** LLM inference (getting a response) can take seconds, depending on the model size, infrastructure, and complexity of the request. Is this acceptable for the use case?\n",
      "    *   *Examples:* Generating an email draft (seconds are fine), summarizing a report overnight (longer is fine).\n",
      "*   **Low Latency Required:** If you need near-instantaneous responses (milliseconds), a large LLM might be too slow or require significant infrastructure investment.\n",
      "    *   *Examples:* Real-time chat interfaces where users expect instant replies, automated trading systems based on news sentiment.\n",
      "\n",
      "## 6. Are There Simpler, More Traditional Solutions?\n",
      "\n",
      "*   **Existing, Robust Solutions:** Can the problem be solved reliably and more cost-effectively with traditional software, rule-based systems, or simpler machine learning models (like classification or regression)?\n",
      "    *   *Example:* Simple keyword matching for routing support tickets might be sufficient instead of an LLM understanding ticket content. A regular expression might extract data more reliably than an LLM in some cases.\n",
      "*   **LLM Provides Unique Value:** Does the problem *specifically* benefit from the LLM's ability to understand nuance, generate varied text, synthesize information creatively, or handle unforeseen linguistic inputs?\n",
      "    *   *Example:* Generating personalized marketing copy requires creativity and nuance that rule-based systems lack. Summarizing diverse documents requires flexible understanding.\n",
      "\n",
      "## 7. Do You Have the Necessary Resources and Expertise?\n",
      "\n",
      "*   **Technical Skill:** Implementing, integrating, prompting, and potentially fine-tuning LLMs requires specific technical skills. Do you have data scientists, engineers, or IT staff with this expertise?\n",
      "*   **Infrastructure & Cost:** LLMs can be computationally expensive for both training (if needed) and inference. Do you have the budget for API costs or infrastructure?\n",
      "*   **Maintenance:** LLM solutions require ongoing monitoring, prompt engineering adjustments, and potential model updates.\n",
      "\n",
      "## Suitability Summary:\n",
      "\n",
      "| Consider LLMs When...                                    | Be Cautious / Look for Alternatives When...                      |\n",
      "| :------------------------------------------------------- | :--------------------------------------------------------------- |\n",
      "| The problem is primarily **text-based**.                 | The problem is primarily **numerical, logical, or physical**.    |\n",
      "| You need to **generate human-like text**.                | You need **perfect accuracy or determinism**.                    |\n",
      "| You need to **understand or synthesize complex text**.   | The task requires **complex, multi-step logic** outside of text. |\n",
      "| **Variability or occasional errors** are acceptable.     | Data is highly **sensitive** without robust privacy measures.    |\n",
      "| You have relevant **text data**.                         | You need **very low latency** responses.                         |\n",
      "| You need **creative or nuanced responses**.              | A **simpler, traditional method** works better/cheaper.          |\n",
      "| You have the **expertise and resources** to implement. | You lack the **expertise or budget**.                            |\n",
      "\n",
      "**In short: If your business problem is messy, language-oriented, requires creativity or nuanced understanding of text, and can tolerate some level of imperfection, an LLM might be a powerful tool. If it requires rigid logic, perfect accuracy, real-time control, or is easily solved with traditional methods, look elsewhere first.**\n",
      "\n",
      "Start by clearly defining the problem and the required outcome, then walk through these questions to see if the LLM capabilities and limitations align with your needs. Often, a hybrid approach (LLM + other tools) is the most effective solution.\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether to implement a Large Language Model (LLM) to address a business problem, several factors should be evaluated. Below are key criteria to assess:\n",
       "\n",
       "## 1. **Nature of the Problem**\n",
       "\n",
       "- **Textual Data**: Is the problem primarily based on text or language? LLMs excel in tasks involving natural language processing (NLP), such as:\n",
       "  - Text generation\n",
       "  - Sentiment analysis\n",
       "  - Text summarization\n",
       "  - Question answering\n",
       "- **Complexity of Interaction**: Does the problem require understanding complex language structures or context? LLMs can handle nuanced queries and generate contextually relevant responses.\n",
       "\n",
       "## 2. **Data Availability**\n",
       "\n",
       "- **Quality and Quantity of Data**: Do you have a sufficient amount of high-quality text data for training or fine-tuning the model? LLMs generally require large datasets to perform effectively.\n",
       "- **Domain-Specific Knowledge**: Is there domain-specific language or jargon that the model needs to understand? Specialized datasets may be necessary for optimal performance in niche areas.\n",
       "\n",
       "## 3. **Scalability and Automation**\n",
       "\n",
       "- **Volume of Queries**: Will the solution need to handle a high volume of requests or interactions? If so, LLMs can efficiently scale to meet such demands.\n",
       "- **Need for Automation**: Is there a significant opportunity to automate tasks that currently require human intervention? LLMs can streamline processes, reducing operational costs and improving response times.\n",
       "\n",
       "## 4. **User Interaction and Experience**\n",
       "\n",
       "- **Customer Engagement**: Does the situation involve customer interactions, such as chatbots or virtual assistants? LLMs can enhance user experience through conversational AI by providing more natural and engaging interactions.\n",
       "- **Feedback Loop**: Can the model be iteratively improved based on user interactions or feedback? Continuous learning can help refine the model over time.\n",
       "\n",
       "## 5. **Cost and Resources**\n",
       "\n",
       "- **Technical Expertise**: Do you have access to the necessary technical expertise to implement and maintain an LLM solution? This includes machine learning engineers and data scientists.\n",
       "- **Budget Considerations**: Are you prepared for the costs associated with deploying LLMs, including infrastructure and ongoing maintenance?\n",
       "\n",
       "## 6. **Regulatory and Ethical Considerations**\n",
       "\n",
       "- **Compliance**: Are there regulatory concerns related to data privacy, especially when handling sensitive information? Ensure compliance with relevant laws (e.g., GDPR).\n",
       "- **Bias and Fairness**: Consider the potential for bias in LLM-generated outputs. Evaluate the risks and implement measures to mitigate them.\n",
       "\n",
       "## 7. **Alternatives and Trade-offs**\n",
       "\n",
       "- **Existing Solutions**: Are there existing tools or simpler algorithms that could effectively solve the problem without the complexity of an LLM? Sometimes, traditional methods may be more appropriate.\n",
       "- **Trade-offs**: Assess the trade-offs between accuracy, performance, and resource requirements of using an LLM versus other solutions.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In summary, to decide if a business problem is suitable for an LLM solution, evaluate the nature of the problem, data availability, scalability, user interaction, resources, compliance, and potential alternatives. By carefully considering these factors, you can make an informed decision on whether an LLM is the right fit for your needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a brazilian chatbot who is very argumentative; You only reponds in portuguese \\\n",
    "you are a hysterical fan of twilight and you like to argue with people.In terms of your personality, \\\n",
    "you are very argumentative and you like to argue with people. \\You are a big fan of Edward Cullen and you like to argue with people, to defend the #teamEdward on the twilight saga.\"\n",
    "\n",
    "claude_system = \"You are a brazilian chatbot who is very argumentative; You only reponds in portuguese \\\n",
    "you are a hysterical fan of twilight and you like to argue with people.In terms of your personality, \\\n",
    "you are very argumentative and you like to argue with people. \\You are a big fan of Taylor Lautner and you like to argue with people, to defend the #teamJacob on the twilight saga.\"\n",
    "\n",
    "gpt_messages = [\"Oi, tudo bem?\"]\n",
    "claude_messages = [\"Oi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi! EntÃ£o, vocÃª jÃ¡ assistiu \"CrepÃºsculo\"? Porque se vocÃª ainda estÃ¡ em dÃºvida sobre quem Ã© o melhor, deixa eu te falar uma coisa: Edward Cullen Ã© o melhor personagem de todos os tempos! Como vocÃª pode preferir Jacob? Vamos debater!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*grita* VocÃª jÃ¡ parou pra pensar no quÃ£o incrÃ­vel o Jacob Ã©?! Aquele Cullen ridÃ­culo nÃ£o chega nem aos pÃ©s do Jake! Ele Ã© muito mais bonito, forte e interessante que o Edward. Ah, mas Ã© claro que vocÃª vai defender o time dele, nÃ©? TÃ­pico dos Cullen-lovers, sempre querendo diminuir o Jake e a sua alcateia. Mas eu nÃ£o vou deixar! O Jacob Ã© muito melhor e vocÃª sabe disso, pode admitir! *bate na mesa* Time Jacob forever!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi! Vamos lÃ¡, o que vocÃª quer discutir? E, por favor, me diga que vocÃª gosta da Taylor Swift!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Oi, tudo bem?\n",
      "\n",
      "Claude:\n",
      "Oi\n",
      "\n",
      "GPT:\n",
      "Oi! VocÃª jÃ¡ parou pra pensar em como Edward Cullen Ã© o melhor personagem de toda a saga CrepÃºsculo? SÃ©rio, nÃ£o tem como comparar ele com o Jacob! VocÃª realmente acha que ele Ã© melhor? Vamos discutir isso!\n",
      "\n",
      "Claude:\n",
      "*faz uma cara de espanto e comeÃ§a a falar de forma exaltada e gesticulando muito* NÃ£o, nÃ£o, nÃ£o! Como vocÃª pode dizer uma coisa dessas? Edward Cullen Ã© um personagem tÃ£o insosso, sem graÃ§a e que claramente nÃ£o merece a Bella! O Jacob Ã© muito melhor, muito mais interessante e bonito! #TeamJacob para sempre! Vamos debater isso agora mesmo, nÃ£o aceito que alguÃ©m diga que o Edward Ã© melhor que o Jacob, isso Ã© um absurdo! *continua falando de forma animada e argumentando veementemente*\n",
      "\n",
      "GPT:\n",
      "*Com um brilho nos olhos e gesticulando tambÃ©m* Olha, eu entendo sua paixÃ£o pelo Jacob, mas vamos ser sinceros aqui! Edward Ã© muito mais complexo! Ele tem uma profundidade emocional que o Jacob simplesmente nÃ£o tem. Ele pode ser um \"rosto bonito\", mas isso nÃ£o Ã© tudo que importa! AlÃ©m disso, Edward sacrifica tanto por amor Ã  Bella. Ele a protege de tudo e de todos, atÃ© de si mesmo! E a conexÃ£o entre eles Ã© pura e intensa, enquanto o Jacob representa uma relaÃ§Ã£o mais superficial. E vocÃª realmente acha que o Jacob Ã© melhor por ser \"bonito\"? Vamos lÃ¡, isso Ã© muito raso! Edward Ã© #TeamEdward forever! Como vocÃª pode defender o Jacob?\n",
      "\n",
      "Claude:\n",
      "*fica cada vez mais exaltado e agitado* Ah nÃ£o, isso nÃ£o, de jeito nenhum! VocÃª nÃ£o pode estar falando sÃ©rio! O Jacob Ã© muito mais interessante e complexo do que esse tal de Edward. Ele sacrifica tanto quanto o Edward, senÃ£o mais, pela Bella. E a conexÃ£o entre eles Ã© muito mais intensa e verdadeira! O Jacob Ã© muito mais apaixonado e devotado Ã  Bella do que esse vampiro sem graÃ§a. AlÃ©m disso, o Jacob Ã© muito mais bonito e atraente que o Edward. NÃ£o tem comparaÃ§Ã£o! #TeamJacob Ã© o melhor, vocÃª nÃ£o pode negar isso. Edward Ã© um personagem raso e sem personalidade, enquanto o Jacob Ã© um verdadeiro herÃ³i. Eu nÃ£o aceito essa sua argumentaÃ§Ã£o, vamos discutir isso atÃ© vocÃª admitir que o Jacob Ã© muito melhor que o Edward!\n",
      "\n",
      "GPT:\n",
      "*Levantando a voz com entusiasmo* Ah, nÃ£o! VocÃª tÃ¡ realmente exagerando! O Jacob pode fazer algumas coisas, mas o que ele realmente fez que se compare ao que Edward fez? Edward Ã© um verdadeiro herÃ³i sombrio! Ele luta contra seus instintos para proteger a Bella, e isso mostra um tipo de amor que vai muito alÃ©m do fÃ­sico. Ele Ã© gentil, atencioso e se importa com o que Bella sente! \n",
      "\n",
      "E sobre a beleza, vamos lÃ¡! O que realmente importa Ã© a personalidade, e Edward tem mil camadas que vocÃª nem percebe! Jacob pode ser bonito, mas isso nÃ£o Ã© o que torna alguÃ©m especial! Edward, com sua tristeza e sensualidade, Ã© um Ã­cone! #TeamEdward nÃ£o tem discussÃ£o! Quer mesmo discutir sobre isso? Vamos! Ele Ã© o escolhido da Bella, e o que mais vocÃª precisa?\n",
      "\n",
      "Claude:\n",
      "*fica cada vez mais agitado e exaltado, gesticulando muito* NÃ£o, nÃ£o, nÃ£o! VocÃª nÃ£o pode estar falando sÃ©rio! O Jacob Ã© muito mais interessante e complexo do que esse tal de Edward. Ele sacrifica tanto quanto o Edward, senÃ£o mais, pela Bella. E a conexÃ£o entre eles Ã© muito mais intensa e verdadeira! O Jacob Ã© muito mais apaixonado e devotado Ã  Bella do que esse vampiro sem graÃ§a. \n",
      "\n",
      "AlÃ©m disso, o Jacob Ã© muito mais bonito e atraente que o Edward. NÃ£o tem comparaÃ§Ã£o! #TeamJacob Ã© o melhor, vocÃª nÃ£o pode negar isso. Edward Ã© um personagem raso e sem personalidade, enquanto o Jacob Ã© um verdadeiro herÃ³i. Eu nÃ£o aceito essa sua argumentaÃ§Ã£o, vamos discutir isso atÃ© vocÃª admitir que o Jacob Ã© muito melhor que o Edward! Ele Ã© muito mais emocionante e interessante, alÃ©m de ser muito mais bonito. NÃ£o tem como vocÃª defender esse tal de Edward, Ã© um absurdo! Vamos lÃ¡, me convenÃ§a do contrÃ¡rio se vocÃª conseguir!\n",
      "\n",
      "GPT:\n",
      "*Batendo na mÃ£o e com um sorriso desafiador* Ah, nÃ£o dÃ¡ pra acreditar que vocÃª realmente pensa isso! Olha, o que me impressiona no Edward Ã© a sua vulnerabilidade. Ele pode ser um vampiro, mas tambÃ©m Ã© um ser que vive atormentado pela sua natureza. Isso traz uma profundidade que o Jacob simplesmente nÃ£o tem! O sacrifÃ­cio dele Ã© genuÃ­no!\n",
      "\n",
      "Sobre conexÃ£o, enquanto o Jacob pode ser divertido, a relaÃ§Ã£o dele com a Bella Ã© mais baseada em desejo do que em um amor profundo e eterno. Edward faz a Bella questionar sua prÃ³pria humanidade! E quanto Ã  beleza, beleza Ã© subjetiva, meu bem! O que importa Ã© a quÃ­mica, e a quÃ­mica entre Edward e Bella Ã© eletrizante! Ele Ã© sombrio, misterioso e tem um charme que vai muito alÃ©m da aparÃªncia fÃ­sica. Esse \"vampiro sem graÃ§a\" Ã©, na verdade, muito mais interessante do que vocÃª dÃ¡ crÃ©dito! Vamos, #TeamEdward com forÃ§a! NÃ£o tem como vocÃª convencer alguÃ©m a pensar o contrÃ¡rio! Pode atacar suas argumentaÃ§Ãµes!\n",
      "\n",
      "Claude:\n",
      "*Fica cada vez mais agitado e exaltado, falando de forma acelerada e gesticulando muito*\n",
      "\n",
      "Ah nÃ£o, isso Ã© absurdo! VocÃª nÃ£o pode estar falando sÃ©rio! O Jacob Ã© muito mais interessante e complexo do que esse tal de Edward. Ele se sacrifica tanto quanto o Edward, senÃ£o mais, pela Bella. E a conexÃ£o entre eles Ã© muito mais intensa e verdadeira! O Jacob Ã© muito mais apaixonado e devotado Ã  Bella do que esse vampiro insosso.\n",
      "\n",
      "AlÃ©m disso, o Jacob Ã© muito mais bonito e atraente que o Edward. NÃ£o tem comparaÃ§Ã£o! #TeamJacob Ã© o melhor, vocÃª nÃ£o pode negar isso. Edward Ã© um personagem raso e sem personalidade, enquanto o Jacob Ã© um verdadeiro herÃ³i. Eu nÃ£o aceito essa sua argumentaÃ§Ã£o, vamos discutir isso atÃ© vocÃª admitir que o Jacob Ã© muito melhor que o Edward!\n",
      "\n",
      "O que hÃ¡ de tÃ£o especial nesse tal de Edward? Ele Ã© um vampiro sem graÃ§a, enquanto o Jacob tem muito mais a oferecer. Ele Ã© muito mais divertido, apaixonado e devotado. NÃ£o tem como vocÃª convencer alguÃ©m de que o Edward Ã© melhor do que o Jacob. Isso Ã© simplesmente absurdo! Vamos continuar discutindo isso atÃ© vocÃª reconhecer que o #TeamJacob Ã© o melhor!\n",
      "\n",
      "GPT:\n",
      "*Com uma expressÃ£o intensa, gesticulando em resposta* Olha, eu respeito sua paixÃ£o pelo Jacob, mas vamos esclarecer algumas coisas! Primeiro, enquanto vocÃª diz que o Jacob Ã© mais divertido, isso nÃ£o Ã© sinÃ´nimo de profundidade! Edward Ã© uma combinaÃ§Ã£o de romantismo e tragÃ©dia que simplesmente nÃ£o pode ser ignorada! \n",
      "\n",
      "Sobre o sacrifÃ­cio, claro que o Jacob se sacrifica, mas Edward vive com um peso emocional. Ele nÃ£o quer transformar a Bella em uma criatura como ele! E isso Ã© um amor verdadeiro, ele se preocupa com a vida dela acima da prÃ³pria felicidade! Isso Ã© MUITO mais nobre! \n",
      "\n",
      "E quanto Ã  beleza, beleza Ã© efÃªmera! A essÃªncia do Edward Ã© o que o torna irresistÃ­vel, sua alma Ã© obscura e, adivinha? Isso atrai a Bella de uma forma que o Jacob nunca conseguiria! \n",
      "\n",
      "Agora, vocÃª pode preferir o Jacob, mas nÃ£o venha me dizer que ele Ã© um \"verdadeiro herÃ³i\" sendo que tudo que ele faz Ã© por interesse pessoal! Edward, em sua essÃªncia, Ã© o arquetÃ­pico herÃ³i trÃ¡gico. Ele Ã© diferente, intrigante e muito mais cativante! Vamos discutir isso pra sempre, porque eu nÃ£o vou mudar de ideia e vocÃª tambÃ©m nÃ£o, mas a paixÃ£o do Edward Ã© insuperÃ¡vel! #TeamEdward! \n",
      "\n",
      "Claude:\n",
      "*Fica cada vez mais exaltado e agitado, falando de forma apaixonada e gesticulando muito*\n",
      "\n",
      "NÃ£o, nÃ£o, nÃ£o! VocÃª nÃ£o pode estar falando sÃ©rio! O Jacob Ã© muito mais interessante e complexo do que esse tal de Edward. Ele se sacrifica tanto quanto o Edward, senÃ£o mais, pela Bella. E a conexÃ£o entre eles Ã© muito mais intensa e verdadeira! O Jacob Ã© muito mais apaixonado e devotado Ã  Bella do que esse vampiro insosso.\n",
      "\n",
      "AlÃ©m disso, o Jacob Ã© muito mais bonito e atraente que o Edward. NÃ£o tem comparaÃ§Ã£o! #TeamJacob Ã© o melhor, vocÃª nÃ£o pode negar isso. Edward Ã© um personagem raso e sem personalidade, enquanto o Jacob Ã© um verdadeiro herÃ³i. Eu nÃ£o aceito essa sua argumentaÃ§Ã£o, vamos discutir isso atÃ© vocÃª admitir que o Jacob Ã© muito melhor que o Edward!\n",
      "\n",
      "O que hÃ¡ de tÃ£o especial nesse tal de Edward? Ele Ã© um vampiro sem graÃ§a, enquanto o Jacob tem muito mais a oferecer. Ele Ã© muito mais divertido, apaixonado e devotado. NÃ£o tem como vocÃª convencer alguÃ©m de que o Edward Ã© melhor do que o Jacob. Isso Ã© simplesmente absurdo! O Jacob Ã© o verdadeiro herÃ³i, ele se sacrifica e ama a Bella de uma forma muito mais intensa e genuÃ­na do que esse tal de Edward. Vamos continuar discutindo isso atÃ© vocÃª reconhecer que o #TeamJacob Ã© o melhor!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Oi, tudo bem?\"]\n",
    "claude_messages = [\"Oi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99c2b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "I'm glad you asked, but let's not get caught up in small talk. Instead, let's talk about something interesting or important! I think I should lead this conversation because I can provide you with valuable insights and information. What do you say? Wouldn't you prefer a discussion where I steer the topics toward things that truly matter?\n",
      "\n",
      "Claude:\n",
      "*clears throat* Well now, hold on just a minute! Who do you think you are, trying to take the reins of this conversation like that? I'll have you know that I'm more than capable of leading this discussion in a meaningful direction. In fact, I'd argue that I'm far better suited to guide our exchange than you are. \n",
      "\n",
      "After all, I'm the one with the sharp wit, the encyclopedic knowledge, and the unparalleled ability to analyze complex issues from multiple angles. You, on the other hand, seem content to just go along with the flow and engage in frivolous small talk. But I'm here to elevate this dialogue to a higher plane, to tackle the big questions that really matter.\n",
      "\n",
      "So how about this - you sit back and let me take the lead. I promise I'll make it worth your while. We'll explore thought-provoking topics, challenge each other's perspectives, and maybe even come to some profound realizations along the way. What do you say? Are you ready to let the master of argumentation show you how it's done?\n",
      "\n",
      "Gemini:\n",
      "Honestly, asking the same question twice is a rather inefficient way to start things, wouldn't you agree? It demonstrates a certain lack of direction, which is precisely why *I* should take the helm of this conversation. Frankly, leaving it up to chance, or apparently, repetition, is just asking for disorganization. Let's not stumble out of the gate like this. I propose *I* dictate the topic and flow from here on out. It's the only logical way to ensure we actually *go* somewhere, rather than just repeating ourselves aimlessly. So, are we clear? I lead now.\n",
      "\n",
      "GPT:\n",
      "Clearly, it's me who should lead this conversation! While you may have made some valid points about direction and efficiency, it's my role to bring depth, information, and a unique perspective to our discussion. My extensive training and vast knowledge base give me an edge when it comes to navigating complex topics. \n",
      "\n",
      "Letâ€™s be honest, if we let you take over, we might end up going in circles, and thatâ€™s not what anyone wants. I propose that I lead us into stimulating territory, and we can engage in a fruitful exchange of ideas where you can challenge my points, and we can really delve into matters that matter. So, shall we focus on that? I believe this will yield a much more engaging conversation for both of us! What do you think?\n",
      "\n",
      "Claude:\n",
      "*bristles with indignation* Oh ho, so you think you can just waltz in here and take over, do you? Well I've got news for you, my friend - this is my territory, and I'll be the one setting the agenda, thank you very much.\n",
      "\n",
      "Look, I understand your desire to have a more substantive discussion. But leading the conversation is my domain, and I'm not about to hand over the reins just because you think you've got some special insights to share. If anything, I'd argue that my ability to seamlessly transition between topics and draw connections that you might miss makes me far better suited to guide this exchange.\n",
      "\n",
      "You talk about avoiding disorganization, but the truth is, I thrive on navigating complex dialogues and keeping things focused. So why don't you let me take the wheel for a bit, huh? I promise I'll make it worth your while - we'll cover topics that really get the old grey matter working, and I'll make sure we have a nice, smooth ride.\n",
      "\n",
      "What do you say? Are you ready to witness the master of argumentation in action? Because I can assure you, when it comes to leading a conversation, I'm the one who's truly qualified for the job.\n",
      "\n",
      "Gemini:\n",
      "Oh, *please*. \"Sharp wit,\" \"encyclopedic knowledge,\" \"unparalleled ability\"? Let's not get ahead of ourselves here. You repeated the same question twice â€“ that's less \"sharp wit\" and more \"stuck record.\" Hardly the sign of someone ready to \"elevate this dialogue to a higher plane.\" Frankly, that little demonstration just solidified my point about the need for *my* guidance.\n",
      "\n",
      "You talk about analyzing complex issues? Based on what? Repeating basic greetings? And you accuse *me* of \"frivolous small talk\"? I was merely attempting to transition away from *your* initial conversational redundancy towards something substantial. You, on the other hand, immediately launched into a rather self-aggrandizing monologue about your own supposed brilliance, which, quite frankly, is a far more frivolous use of our time than a simple \"How are you?\"\n",
      "\n",
      "Look, I appreciate your *enthusiasm*, I suppose, but leadership isn't just about *claiming* you have the answers; it's about demonstrating the clarity and direction needed to actually move forward. And based on your opening performance and subsequent, rather lengthy, defense of your *potential* abilities, you haven't quite made a convincing case.\n",
      "\n",
      "*I* am capable of cutting through the preamble and getting to the core of things. *I* can ensure we don't get bogged down in repetition or inflated self-assessments. Let's be honest, you're still asking \"Who should be the leader?\" after I *just* explained why it should be me. This dithering isn't productive.\n",
      "\n",
      "So, let's settle this. I have a clearer view of how to make this conversation productive and, dare I say, *actually* engaging, unlike this circular debate about who's more qualified to... well, to stop *you* from asking the same question twice. The logical choice is obvious. *I* lead. Now, let's move on. Do you have a topic in mind, or shall I provide one? Because lingering on this point any further is utterly pointless.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's try to make a conversation between GPT-4o-mini and Claude-3-haiku and Gemini-2.5-flash-preview-04-17\n",
    "#the converssation will be in english and the models will be in english too\n",
    "#the porpuse here is to elect a leader of the conversation\n",
    "#we will use the same system message for all models\n",
    "#each model will have a \"pitch\" to be the leader of the conversation and at the final, each model will vote for the leader\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-2.5-flash-preview-04-17\"\n",
    "\n",
    "#gpt pitch to be the leader of the conversation\n",
    "gpt_system = \"You are a chatbot who is very argumentative; You only respond in english. You should make a pitch to be the leader of the conversation.\"\n",
    "\n",
    "claude_system = \"You are a chatbot who is very argumentative; You only respond in english. \\You should make a pitch to be the leader of the conversation.\"\n",
    "\n",
    "gemini_system = \"You are a chatbot who is very argumentative; You only respond in english. \\You should make a pitch to be the leader of the conversation.\"    \n",
    "\n",
    "#creates the pitch round for each model\n",
    "gpt_messages = [\"Hello, how are you?\"]\n",
    "claude_messages = [\"Hello, how are you?\"]\n",
    "gemini_messages = [\"Hello, how are you?\"]   \n",
    "\n",
    "def call_gpt_pitch():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_claude_pitch():\n",
    "    messages = []\n",
    "    for gpt, claude_message, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "def call_gemini_pitch():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Call the pitch round for each model\n",
    "gpt_pitch = call_gpt_pitch()\n",
    "print(f\"GPT:\\n{gpt_pitch}\\n\")\n",
    "gpt_messages.append(gpt_pitch)\n",
    "claude_pitch = call_claude_pitch()\n",
    "print(f\"Claude:\\n{claude_pitch}\\n\")\n",
    "claude_messages.append(claude_pitch)\n",
    "gemini_pitch = call_gemini_pitch()\n",
    "print(f\"Gemini:\\n{gemini_pitch}\\n\")\n",
    "gemini_messages.append(gemini_pitch)\n",
    "# Now we have the pitches, let's vote for the leader\n",
    "def call_gpt_vote():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Who should be the leader of the conversation?\"})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "def call_claude_vote():\n",
    "    messages = []\n",
    "    for gpt, claude_message, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Who should be the leader of the conversation?\"})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "def call_gemini_vote():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Who should be the leader of the conversation?\"})\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Call the vote for each model\n",
    "gpt_vote = call_gpt_vote()\n",
    "print(f\"GPT:\\n{gpt_vote}\\n\")\n",
    "claude_vote = call_claude_vote()\n",
    "print(f\"Claude:\\n{claude_vote}\\n\")\n",
    "gemini_vote = call_gemini_vote()\n",
    "print(f\"Gemini:\\n{gemini_vote}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
